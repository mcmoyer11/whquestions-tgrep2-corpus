---
title: "Pre-processing Data from Embedded Questions NoContext Corpus Study"
author: Morgan Moyer
date: May 18, 2021
output: html_document
---

```{r setup, include=FALSE, warning=FALSE, echo=FALSE}
library(ggplot2)
library(lme4)
library(lmerTest)
library(multcomp) # not available for this version of R
library(stringr)
library(textstem)
library(tidyverse)
theme_set(theme_bw())
cbPalette <- c("#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73","#56B4E9", "#D55E00", "#009E73","#999999", "#E69F00","#009E73")
this.dir <- dirname(rstudioapi::getSourceEditorContext()$path)
setwd(this.dir)
source("../../helpers.R")
```


```{r import data}
# Read database
corp = read.table("../../../corpus/results/swbd.tab",sep="\t",header=T,quote="")
# Read experimental data in
d = read.csv("../../05_experiment/data/exp0506-merged.csv")
d <- d %>%
  filter(.,grepl("experiment_06",proliferate.condition))
nrow(d) # 108995
```

```{r combine raw data with corpus file,warning=FALSE}
# rename item_id from corpus file
names(corp)[names(corp) == "Item_ID"] <- "tgrep_id"

# filter from the database the tgrep_ids from the raw data
corp_match = corp %>%
  filter(tgrep_id %in% d$tgrep_id)

# join dfs together
d <- left_join(d, corp_match, by="tgrep_id")
# assign the negative values 0
d$rating[d$rating < 0] = 0
# table(d$proliferate.condition)
```

```{r take a look at the raw distribution, include=FALSE, echo=FALSE}
ggplot(d, aes(x=rating)) +
  geom_histogram()

```


# Native speakers
## We exclude any speakers who did not report English as a native language in the post-survey demographic questions.
```{r Filter out non-native English speakers, warning = FALSE}
nrow(d) # 104686
length(unique(d$workerid)) # 717
#filter out participants who did not report English as native language
d <- d %>%
  mutate(language = tolower(subject_information.language)) %>%
  filter(., grepl("eng", language))

# 28 participants removed for reporting non-English native language
nrow(d) #104686
length(unique(d$workerid)) # 689
```

```{r comments and demographic information, eval=FALSE, include=FALSE}
d$time_in_minutes = as.numeric(as.character(d$time_in_minutes))
d$rating = as.numeric(d$rating)

# look at comments
unique(d$subject_information.comments)

# fair price
ggplot(d, aes(x=subject_information.fairprice)) +
  geom_histogram(stat="count")

# overall assessment
ggplot(d, aes(x=subject_information.enjoyment)) +
  geom_histogram(stat="count")

# gender
ggplot(d, aes(x=subject_information.gender)) +
  geom_histogram(stat="count")

# education
ggplot(d, aes(x=subject_information.education)) +
  geom_histogram(stat="count")

# time_in_minutes
ggplot(d, aes(x=time_in_minutes)) +
  geom_histogram(stat="count")
mean(d$time_in_minutes)
```

```{r Look at practice trials, include = FALSE}
practice = d %>%
  filter(tgrep_id %in% c("example1", "example2", "example3", "example4"))

agr = practice %>%
  group_by(tgrep_id, paraphrase) %>%
  summarize(mean_rating = mean(rating), CILow = ci.low(rating), CIHigh = ci.high(rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

labels = c("Who can help spread the word?","Where can I get coffee around here?","Who came to the party?","How do I get to Central Park?")
names(labels) = c("example1","example2","example3", "example4")

ggplot(agr,aes(x=paraphrase, y=mean_rating, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position="dodge") +
  facet_wrap(~tgrep_id,labeller = labeller(tgrep_id = labels))
# ggsave("../graphs/main_practice_total.pdf")
# theme(axis.text.x = element_text(angle = 90))
```

```{r TODO: look at first response on practice trials only, eval=FALSE, include=FALSE}
prac_agr = practice %>%
  group_by(workerid,tgrep_id,paraphrase,rating) %>%
  summarise(count = n()) %>%
  group_by(workerid,tgrep_id) %>%
  mutate(total_per_ex = sum(count))
nrow(prac_agr) # 19566

prac_agr_rem = practice %>%
  group_by(workerid,tgrep_id,paraphrase,rating) %>%
  summarise(count = n()) %>%
  group_by(workerid,tgrep_id) %>%
  mutate(total_per_ex = sum(count)) %>%
  filter(total_per_ex > 4)
  # write.csv(.,"practice_to_edit.csv")
nrow(prac_agr_rem) # 2014

# fixed = read.csv("practice_edited.csv",header=TRUE)
nrow(fixed) # 68
head(fixed)
# remove that one column
# fixed = fixed[c(2:7)]

head(fixed)
prac_agr_keep = practice %>%
  group_by(workerid,tgrep_id,paraphrase,rating) %>%
  summarise(count = n()) %>%
  group_by(workerid,tgrep_id) %>%
  mutate(total_per_ex = sum(count)) %>%
  filter(total_per_ex <= 4)
nrow(prac_agr_keep) # 4088

practice_first = rbind(fixed,prac_agr_keep)
nrow(practice_first) #320
```

```{r Look at control trials}
controls = d %>%
  filter((grepl("control",tgrep_id)) & (rating >= 0))

# read in the file to have access to the items
cntrls = read.csv("../../../experiments/clean_corpus/controls.csv",header=TRUE,quote="")

# rename the item column in order to merge on it
names(cntrls)[names(cntrls) == "TGrepID"] <- "tgrep_id"

# join dfs together
c <- left_join(controls, cntrls, by="tgrep_id")
```

```{r Graph control trials, eval=FALSE, include=FALSE}
agr = c %>%
  group_by(EntireSentence,paraphrase) %>%
  summarize(mean_rating = mean(rating), CILow = ci.low(rating), CIHigh = ci.high(rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

ggplot(agr,aes(x=paraphrase, y=mean_rating, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position="dodge") +
  facet_wrap(~EntireSentence, labeller = labeller(Sentence = label_wrap_gen(1)))
# ggsave("../graphs/main_controls.pdf")
```

# Remove Subjects who failed 2/6 Controls
```{r Remove subjects who failed 2/6 controls}
# for each control trial type, create a binary measure of whether the
# trial was passed
t = c %>%
  separate(tgrep_id,into=c("tgrep_id","para","trial"),sep="[_]") %>%
  group_by(workerid,paraphrase,trial) %>%
  filter(trial %in% c("movie", "book"), paraphrase %in% c("the")) %>%
  mutate(control_passed = ifelse(rating > .5,"1","0"))
# nrow(t) #2028
a1 = c %>%
  separate(tgrep_id,into=c("tgrep_id","para","trial"),sep="_") %>%
  group_by(workerid,paraphrase,trial) %>%
  filter(trial %in% c("novels", "cookies"), paraphrase %in% c("all")) %>%
  mutate(control_passed = ifelse(rating > .5,"1","0"))
# nrow(a1) #2028
a2 = c %>%
  separate(tgrep_id,into=c("tgrep_id","para","trial"),sep="_") %>%
  group_by(workerid,paraphrase,trial) %>% 
  filter(trial %in% c("tissue", "napkin"), paraphrase %in% c("a")) %>%
  mutate(control_passed = ifelse(rating > .5,"1","0"))
# nrow(a2) #2028

# combine all those files together
con = rbind(t,a1,a2)
# nrow(con) #3936

# filter out participants who failed more than 2 controls by taking the sum of 
# all the passed controls, and filtering out workerids who passed more than 2
failed_controls = con %>%
  filter(control_passed == "1") %>%
  group_by(workerid, control_passed) %>%
  summarise(sum_control_passed = n()) %>%
  filter(sum_control_passed < 4)

# How many removed using this criterion
length(unique(failed_controls$workerid)) # 43
length(unique(failed_controls$workerid))/length(unique(d$workerid))*100 # 6.2%

```

# Look at raw test items 

```{r Look at raw test items}
test = d %>%
  # Remove participants who fail 2/6 controls
  filter(!workerid %in% c(failed_controls$workerid)) %>%
  # Remove non-test items
  filter(!tgrep_id %in% c("example1", "example2", "example3", "example4","bot_check")) %>%
  filter(!grepl("control",tgrep_id)) %>%
  # Make a single matrix verb column
  unite("MatrixVerb", c("MatrixPredVerb", "MatrixPredOther", "MatrixPredParticle"), na.rm = TRUE, sep = " ", remove = FALSE) %>%
  # Remove leading and trailing whitespace
  mutate(MatrixVerb = str_trim(MatrixVerb))

# replace the empty values in MatrixVerb with the value for Aux verb column
test$MatrixVerb = ifelse(test$MatrixVerb=="",test$MatrixPredAux,test$MatrixVerb)

# Lemmatize the MatrixVerb column
test$VerbLemma = lemmatize_words(test$MatrixVerb)

# rename all to every
test$paraphrase[test$paraphrase == "all"] = "every"

# Assign 0 to NaN
test[is.na(test)] <- 0
nrow(test) # 72216
```

## Recode 
* 15 cases where MatrixVerbs weren't printed with TDTLite
* Modals
```{r, Recode}
# 61188:30 --> funny 
test$MatrixVerb[test$tgrep_id == "61188:30"] = "funny"
test$VerbLemma[test$tgrep_id == "61188:30"] = "funny"
test$MatrixPredVerb[test$tgrep_id == "61188:30"] = "funny"

# 4959:7 -- > astonishing
test$MatrixVerb[test$tgrep_id == "4959:7"] = "astonishing"
test$VerbLemma[test$tgrep_id == "4959:7"] = "astonish"

# 99380:19
test$MatrixVerb[test$tgrep_id == "99380:19"] = "close"
test$VerbLemma[test$tgrep_id == "99380:19"] = "close"
test$MatrixPredParticle[test$tgrep_id == "99380:19"] = "close"

	
# 123739:91
test$MatrixVerb[test$tgrep_id == "99380:19"] = "closer"
test$VerbLemma[test$tgrep_id == "99380:19"] = "close"
test$MatrixPredParticle[test$tgrep_id == "99380:19"] = "close"


# 105054:19
test$MatrixVerb[test$tgrep_id == "105054:19"] = "enough"
test$VerbLemma[test$tgrep_id == "105054:19"] = "enough"
test$MatrixPredParticle[test$tgrep_id == "105054:19"] = "enough"

# 147760:69 --> matter of
test$MatrixVerb[test$tgrep_id == "147760:69"] = "matter"
test$VerbLemma[test$tgrep_id == "147760:69"] = "matter"
test$MatrixPredParticle[test$tgrep_id == "147760:69"] = "matter"

	
# 33547:68
test$MatrixVerb[test$tgrep_id == "33547:68"] = "near"
test$VerbLemma[test$tgrep_id == "33547:68"] = "near"
test$MatrixPredOther[test$tgrep_id == "33547:68"] = "near"

# 34493:13 --> no verb
	
# 84861:52 --> away from
test$MatrixVerb[test$tgrep_id == "84861:52"] = "away"
test$VerbLemma[test$tgrep_id == "84861:52"] = "away"
test$MatrixPredParticle[test$tgrep_id == "84861:52"] = "away"

# 63673:21 --> near
test$MatrixVerb[test$tgrep_id == "63673:21"] = "near"
test$VerbLemma[test$tgrep_id == "63673:21"] = "near"


# 126691:17 --> not
test$MatrixVerb[test$tgrep_id == "126691:17"] = "not"
test$VerbLemma[test$tgrep_id == "126691:17"] = "not"
test$MatrixPredOther[test$tgrep_id == "126691:17"] = "not"

# 148174:24 --> is
test$MatrixVerb[test$tgrep_id == "148174:24"] = "is"
test$VerbLemma[test$tgrep_id == "148174:24"] = "be"
test$MatrixPredAux[test$tgrep_id == "148174:24"] = "is"

# 63769:100 --> enough
test$MatrixVerb[test$tgrep_id == "63769:100"] = "enough"
test$VerbLemma[test$tgrep_id == "63769:100"] = "enough"
test$MatrixPredParticle[test$tgrep_id == "63769:100"] = "enough"


# Recode nonfinite clauses as ModalPresent for graphs
test$ModalPresent[test$Finite == "no"] = "yes"
test$Modal[test$Finite == "no"] = "nonfinite"
# Recode contracted Modals
test$Modal[test$Modal == "ca"] = "can"
test$Modal[test$Modal == "'ll"] = "will"
test$Modal[test$Modal == "'d"] = "could"
```

# Graph Raw test items
```{r Graph raw test items, eval=FALSE, include=FALSE}
agr = test %>%
  group_by(paraphrase) %>%
  summarize(mean_rating = mean(rating), CILow = ci.low(rating), CIHigh = ci.high(rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

ggplot(agr,aes(x=paraphrase, y=mean_rating, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position="dodge", show.legend = FALSE) +
  # ggtitle("Overall mean rating for each paraphrase") +
  xlab("Paraphrase") +
  ylab("Mean rating") +
  theme(legend.position = "none") +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)
# ggsave("../graphs/2b_overall_raw.pdf",width=3.5,height=3)
```


```{r}

length(unique(test$MatrixVerb)) # 193 verbs

agr = test %>%
  group_by(Wh,ModalPresent,paraphrase) %>%
  summarize(mean_rating = mean(rating), CILow = ci.low(rating), CIHigh = ci.high(rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

ggplot(agr,aes(x=paraphrase, y=mean_rating, fill=ModalPresent)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  facet_wrap(~Wh) +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)
```

# Remove Rhetorical Questions

```{r Remove rhetorical questions}
test_agr = test %>%
  group_by(tgrep_id, paraphrase) %>%
  summarize(mean_rating = mean(rating))

other_ratings = test %>%
  group_by(tgrep_id, paraphrase) %>%
  summarize(mean_rating = mean(rating)) %>%
  filter((mean_rating[paraphrase == "other"] > mean_rating[paraphrase=="a"]) & 
           (mean_rating[paraphrase == "other"] > mean_rating[paraphrase=="every"]) & 
           (mean_rating[paraphrase == "other"] > mean_rating[paraphrase=="the"]))

# How much data removed
nrow(other_ratings)/nrow(test_agr)*100 # 17.8%
nrow(test_agr) # 4772

or_ids = other_ratings$tgrep_id
test_other = test %>%
  filter(tgrep_id %in% or_ids)
nrow(test_other)/nrow(test)*100 # 18.6%

test_other %>%
  group_by(Sentence,Question,paraphrase,rating) %>%
  summarize(mean_rating = mean(rating)) %>%
  filter(paraphrase == "other") %>%
  View()

# filter out those bad boys
test_norm = test %>%
  filter(!tgrep_id %in% or_ids)

nrow(test_norm)/nrow(test)*100 # 81%

nrow(test_norm) # 58756

# write.csv(test_norm,"../data/no_rhetorical_nocontext_embedded.csv")
```

```{r Look at Matrix Verbs, eval=FALSE, include=FALSE}
verb_count = test %>%
  group_by(VerbLemma) %>%
  summarize(count = n())
View(verb_count)

# take a look at the cases without a verb
no_verb = test %>%
  filter(VerbLemma == "")

test %>%
  filter(VerbLemma == "") %>%
  group_by(tgrep_id) %>%
  summarize(count = n()) %>%
  View()

View(no_verb)
length(unique(no_verb$tgrep_id)) #15 items without verb
View(unique(no_verb[,c("tgrep_id","Sentence","Question")]))
# rename 

```

# Normalize ratings
  
```{r Normalize ratings to make a probability distribution}
# remove 'other' ratings
critical = test_norm %>%
  filter(paraphrase %in% c("every","a","the"))

# unique by-item/by-participant combo
critical$ids = paste(critical$workerid,critical$tgrep_id)

critical$ModalPresent = as.factor(critical$ModalPresent)
critical$Wh = as.factor(critical$Wh)
critical$MatrixVerb = as.factor(critical$MatrixVerb)
critical$paraphrase = as.factor(critical$paraphrase)

# Determine for each observation (by-participant by-item), get the sum of the 
# 3 ratings
cr = critical %>%
  # select(ids,rating) %>%
  group_by(ids) %>%
  summarize(rating_sum = sum(rating))

# Join the dfs together
critical = critical %>%
  left_join(cr, by="ids")

# For each of the three paraphrase ratings, divide by sum of ratings for that item
critical$factors = paste(critical$ids,critical$paraphrase)
normed_agr = critical %>%
  group_by(factors) %>%
  summarise(normed_rating = rating/rating_sum) %>%
  drop_na() # this removes ALOT of rows

# Merge the dfs together
normed = merge(normed_agr,critical,by='factors')
normed[is.na(normed$ModalPresent)] <- "no"

# FIND OUT:
# are there particular items that show bimodality between "other" and another para?

# subset the data to just the relevant columns so that we don't encounter \
# a large file storage problem on GitHub
normed <- normed[,c("workerid","tgrep_id","Sentence","paraphrase","ModalPresent","normed_rating","QuestionType","Finite","Modal","Wh","VerbLemma")]

# save to .csv to load into analysis script
# write.csv(normed,"../data/normed_nocontext_embedded.csv")
```

# Graph normed data

## Overall

```{r Graph normed data Overall}
agr = normed %>%
  group_by(paraphrase) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()


dodge = position_dodge(.9)
ggplot(agr,aes(x=paraphrase, y=mean_rating, fill=paraphrase)) +
  geom_bar(position=dodge,stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=dodge, show.legend = FALSE) +
  # ggtitle("Mean rating for 'a' vs. 'every'") +
  xlab("Paraphrase") +
  ylab("Mean rating") +
  ylim(0,.6) +
  theme(legend.position = "none") +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)
# ggsave("../graphs/overall_nocontext_embedded.pdf",width=2,height=2)
```


## Modal x Wh-Word

```{r Look at Interaction between Modal and Wh}

agr = normed %>%
  group_by(Wh,ModalPresent,paraphrase) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

# Re-Order the WH-levels by overall composition of DB
agr$Wh <- factor(agr$Wh, levels=c("what","how","where","why","who","when"))

ggplot(agr,aes(x=paraphrase, y=mean_rating, alpha=ModalPresent, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  facet_wrap(~Wh, ncol=2) +
  xlab("Paraphrase") +
  ylab("Mean rating") +
  guides(fill=FALSE) +
  guides(alpha=guide_legend(title="Modal present")) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal",
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(0,0,-5,-5),legend.spacing.y = unit(0.001, 'cm')) +
    scale_fill_manual(values=cbPalette) +
    scale_color_manual(values=cbPalette) +
    scale_alpha_discrete(range = c(.5,1))

# ggsave("../graphs/modxwh_nocontext_embedded.pdf",width=3,height=4)
```


```{r Look at just Wh,include=FALSE}
agr = normed %>%
  group_by(paraphrase,Wh) %>%
  summarize(mean_rating = mean(rating), CILow = ci.low(rating), CIHigh = ci.high(rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()
# Re-Order the WH-levels by overall composition of DB
agr$Wh <- factor(agr$Wh, levels=c("what","how","where","why","who","when"))
ggplot(agr,aes(x=Wh, y=mean_rating, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax), width=.25,position=position_dodge(0.9))  +
  # ggtitle("Mean rating for Wh-Word") +
  xlab("Wh-Word") +
  ylab("Mean rating") +
  theme(legend.title = element_blank()) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal") +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)
```

```{r Look at just ModalPresent,include=FALSE}
agr = normed %>%
  group_by(paraphrase,ModalPresent) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

ggplot(agr,aes(x=ModalPresent, y=mean_rating, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax), width=.25,position=position_dodge(0.9)) +
  xlab("Modal Present") +
  ylab("Mean rating") +
  theme(legend.title = element_blank()) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal") +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)

```

```{r Graph MatrixVerb only}
verb_count = normed %>%
  filter(grepl("experiment_06",proliferate.condition)) %>%
  group_by(VerbLemma) %>%
  summarize(count = n())

View(verb_count)
nrow(verb_count) # 92 verbs


agr = normed %>%
  # Most frequent
  filter(VerbLemma %in% c("know","see","wonder","understand","be","understand","surprise","tell","figure","say","think","sure","learn","remember")) %>%
  group_by(paraphrase,VerbLemma,ModalPresent) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()


ggplot(agr,aes(x=paraphrase, y=mean_rating, alpha=ModalPresent, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  facet_wrap(~VerbLemma) +
  xlab("Paraphrase") +
  ylab("Mean rating") +
  guides(fill=FALSE) +
  guides(alpha=guide_legend(title="Modal present")) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal",
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(0,0,-5,-5),legend.spacing.y = unit(0.001, 'cm')) +
    scale_fill_manual(values=cbPalette) +
    scale_color_manual(values=cbPalette) +
    scale_alpha_discrete(range = c(.5,1))

# ggsave("../graphs/matrixverbs_threotical.pdf",width=10,height=10)
```

```{r Look at individual verbs}
agr = normed %>%
  filter(VerbLemma == "know") %>%
  group_by(Wh,ModalPresent,paraphrase) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh) %>%
  drop_na()

# Re-Order the WH-levels by overall composition of DB
agr$Wh <- factor(agr$Wh, levels=c("what","how","where","why","who","when"))

ggplot(agr,aes(x=paraphrase, y=mean_rating, alpha=ModalPresent, fill=paraphrase)) +
  geom_bar(position="dodge",stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,position=position_dodge(0.9)) +
  facet_wrap(~Wh,ncol=2) +
  xlab("Paraphrase") +
  ylab("Mean rating") +
  guides(fill=FALSE) +
  guides(alpha=guide_legend(title="Modal present")) +
  ggtitle("Know") + 
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal",
        legend.margin=margin(0,0,0,0),
        legend.box.margin=margin(0,0,-5,-5),legend.spacing.y = unit(0.001, 'cm')) +
    scale_fill_manual(values=cbPalette) +
    scale_color_manual(values=cbPalette) +
    scale_alpha_discrete(range = c(.5,1))

# ggsave("../graphs/modwh_know.pdf",width=3,height=4)
```

```{r Look at individual modals}
agr = normed %>%   
  filter(ModalPresent %in% c("yes")) %>%
  group_by(Modal,paraphrase) %>%
  summarize(mean_rating = mean(normed_rating), CILow = ci.low(normed_rating), CIHigh = ci.high(normed_rating)) %>%
  mutate(YMin = mean_rating - CILow, YMax = mean_rating + CIHigh)

ggplot(agr, aes(x=paraphrase,y=mean_rating,fill=paraphrase)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax), width=.25,position=position_dodge(0.9)) +
  facet_wrap(~Modal,ncol=3) +
  theme(legend.title = element_blank()) +
  theme(legend.key.size = unit(0.3, "cm"),
        legend.position = "top", # c(.5,1)
        legend.direction = "horizontal") +
  scale_fill_manual(values=cbPalette) +
  scale_color_manual(values=cbPalette)
# ggsave("../graphs/modals.pdf",,width=4,height=5)
```


```{r Looking at individual items}
the_high = normed %>%
  filter((paraphrase == "the") & (VerbLemma == "surprise")) %>% #  & (Wh == "why")
  group_by(tgrep_id,Sentence) %>%
  summarize(mean_rating = mean(normed_rating), sd = sd(normed_rating)) %>%
  filter(mean_rating > .5) %>%
  View()

a_high = normed %>%
  filter((paraphrase == "a") & (VerbLemma == "tell")) %>% #     & (ModalPresent == "no") & (Wh == "who")
  group_by(tgrep_id,Sentence, Question) %>%
  summarize(mean_rating = mean(normed_rating), sd = sd(normed_rating)) %>%
  # filter(mean_rating > .3) %>%
  View()

all_high = normed %>%
  filter((paraphrase %in% c("every") & VerbLemma == "predict")) %>% #  & (Wh == "when") & (ModalPresent == "yes")
  group_by(tgrep_id,Sentence) %>%
  summarize(mean_rating = mean(normed_rating), sd = sd(normed_rating)) %>%
  filter(mean_rating > .1) %>%
  View()

# Look at all ratings for a specific example
ex = test %>%
  filter(tgrep_id == "29177:41") %>%
  select(Sentence,Question,paraphrase,rating) %>%
  group_by(Sentence,Question,paraphrase) %>%
  summarize(mean_rating = mean(rating), sd = sd(rating)) %>%
  View()  

ex = normed %>%
  filter(tgrep_id == "2512:34") %>%
  group_by(Sentence,Question,paraphrase) %>%
  summarize(mean_rating = mean(normed_rating), sd = sd(normed_rating)) %>%
  View()


ex = test %>%
  filter(tgrep_id == "39551:19")

```